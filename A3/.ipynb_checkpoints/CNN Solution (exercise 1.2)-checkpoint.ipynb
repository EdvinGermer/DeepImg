{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ef31d9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611a7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from time import time\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c864bb",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b836693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(batch_size):\n",
    "\n",
    "    root = \"C:/Users/edvin/GitHub/DeepImg/A3/MNIST/\"\n",
    "    \n",
    "    # Load the dataset\n",
    "    train_data = datasets.MNIST(root+\"Train\", train=True, download=False, transform=ToTensor())\n",
    "    test_data = datasets.MNIST(root+\"Test\", train=False, download=False, transform=ToTensor())\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7576b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: torch.Size([100, 1, 28, 28])\n",
      "\n",
      "  Nr of images: 100\n",
      "  Channels: 1\n",
      "  Width: 28\n",
      "  Height: 28\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_mnist(100)\n",
    "for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "    \n",
    "    print(f\"data.shape: {data.shape}\\n\")\n",
    "    \n",
    "    print(f\"  Nr of images: {data.shape[0]}\")\n",
    "    print(f\"  Channels: {data.shape[1]}\")\n",
    "    print(f\"  Width: {data.shape[2]}\")\n",
    "    print(f\"  Height: {data.shape[3]}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf2b4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: torch.Size([10000, 1, 28, 28])\n",
      "\n",
      "  Nr of images: 10000\n",
      "  Channels: 1\n",
      "  Width: 28\n",
      "  Height: 28\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_mnist(100)\n",
    "for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "    \n",
    "    print(f\"data.shape: {data.shape}\\n\")\n",
    "    \n",
    "    print(f\"  Nr of images: {data.shape[0]}\")\n",
    "    print(f\"  Channels: {data.shape[1]}\")\n",
    "    print(f\"  Width: {data.shape[2]}\")\n",
    "    print(f\"  Height: {data.shape[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17f1ac",
   "metadata": {},
   "source": [
    "# Define Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b9f2a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer 1:   8 filters with dim=3x3x1  ->  dim(Z1) = 28x28x8\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1) \n",
    "        \n",
    "        # Layer 2: Max Pooling with dim=2x2    ->  dim(Z2) = 14x14x8\n",
    "        \n",
    "        # Layer 3:   16 filters with dim=3x3x8  ->  dim(Z3) = 14x14x16\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)  # 16 filters: 3x3x8 ->\n",
    "        \n",
    "        # Layer 4: Max Pooling with dim=2x2    ->  dim(Z4) = 7x7x16\n",
    "        \n",
    "        # Layer 5: 32 filters with dim=3x3x16  ->  dim(Z5) =  7x7x32\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) \n",
    "        \n",
    "        # Layer 6: Fully connected with dim=1568x10  ->  dim(Z6) = 1x10\n",
    "        self.W6 = nn.Linear(7 * 7 * 32, 10)   \n",
    "\n",
    "        # Define activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2, stride=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Layer 1\n",
    "        Z1 = self.conv1(X)\n",
    "        A1 = self.relu(Z1)\n",
    "        \n",
    "        # Layer 2\n",
    "        A2 = self.maxpool(A1)\n",
    "        \n",
    "        # Layer 3\n",
    "        Z3 = self.conv2(A2)\n",
    "        A3 = self.relu(Z3)\n",
    "        \n",
    "        # Layer 4\n",
    "        A4 = self.maxpool(A3)\n",
    "        \n",
    "        # Layer 5\n",
    "        Z5 = self.conv3(A4)\n",
    "        A5 = self.relu(Z5)\n",
    "        A5 = A5.reshape(A5.size(0), -1)  # Flatten\n",
    "\n",
    "        # Layer 6\n",
    "        Z6 = self.W6(A5)\n",
    "        Y_hat = self.softmax(Z6)\n",
    "\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d9fd0",
   "metadata": {},
   "source": [
    "# Accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a0525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(output,labels):\n",
    "    # How many data points\n",
    "    n = labels.size(0)\n",
    "    \n",
    "    # Get model predictions\n",
    "    _, predictions = torch.max(output, 1)\n",
    "    \n",
    "    # Vector of \"True\" or \"False\" if prediction matches true labels\n",
    "    correct = (predictions == labels)\n",
    "    \n",
    "    # Count how many \"True\"\n",
    "    total_correct = correct.sum().item()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = total_correct / n\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f537ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(test_loader,model,device):\n",
    "    # Load data and labels\n",
    "    for data, labels in test_loader:  # Test_loader is one batch of all test data\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "    # Feed into model\n",
    "    output = model(data).to(device)\n",
    "    \n",
    "    # Get accuracy by comparing output and labels\n",
    "    acc_test = get_accuracy(output,labels) \n",
    "    \n",
    "    return acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743257d6",
   "metadata": {},
   "source": [
    "# Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ef01976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,test_loader,learning_rate, epochs, device, loss_vec):\n",
    "    \n",
    "    # Define model\n",
    "    model = CNN().to(device)  # Select model and move to \"device\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Performance with random weights\n",
    "    test_acc = test_accuracy(test_loader,model,device)\n",
    "    print(f'Epoch [0/{epochs}],                                  , Test Acc: {test_acc:.5f}')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1,epochs+1):\n",
    "        for idx, (data, labels) in enumerate(train_loader):\n",
    "            # Move to \"device\"\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Model forward and backward\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Save loss every 100 batches\n",
    "            if idx%100==0:\n",
    "                loss_vec.append(loss.item())\n",
    "            \n",
    "        # Accuracy on last mini-batch of train data\n",
    "        acc_train = get_accuracy(output,labels)\n",
    "        \n",
    "        # Accuracy on all test data\n",
    "        acc_test = test_accuracy(test_loader,model,device)  \n",
    "            \n",
    "        # Print epoch results\n",
    "        print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item():.5f}, Train Acc: {acc_train:.5f}, Test Acc: {acc_test:.5f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e82cb7",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef1107",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "\n",
      "Epoch [0/25],                                  , Test Acc: 0.10110\n",
      "Epoch [1/25], Loss: 2.28426, Train Acc: 0.26000, Test Acc: 0.26010\n",
      "Epoch [2/25], Loss: 1.64379, Train Acc: 0.83000, Test Acc: 0.82300\n",
      "Epoch [3/25], Loss: 1.64805, Train Acc: 0.81000, Test Acc: 0.84410\n",
      "Epoch [4/25], Loss: 1.62383, Train Acc: 0.84000, Test Acc: 0.85730\n",
      "Epoch [5/25], Loss: 1.59089, Train Acc: 0.87000, Test Acc: 0.86420\n",
      "Epoch [6/25], Loss: 1.57438, Train Acc: 0.90000, Test Acc: 0.87180\n",
      "Epoch [7/25], Loss: 1.66417, Train Acc: 0.80000, Test Acc: 0.87590\n",
      "Epoch [8/25], Loss: 1.56968, Train Acc: 0.90000, Test Acc: 0.87810\n",
      "Epoch [9/25], Loss: 1.58077, Train Acc: 0.87000, Test Acc: 0.88100\n",
      "Epoch [10/25], Loss: 1.51017, Train Acc: 0.96000, Test Acc: 0.96420\n",
      "Epoch [11/25], Loss: 1.53920, Train Acc: 0.92000, Test Acc: 0.97090\n",
      "Epoch [12/25], Loss: 1.50042, Train Acc: 0.96000, Test Acc: 0.97250\n",
      "Epoch [13/25], Loss: 1.50780, Train Acc: 0.96000, Test Acc: 0.97190\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 100\n",
    "learning_rate = 0.05\n",
    "epochs = 25\n",
    "\n",
    "# List for storing loss\n",
    "loss_vec = []\n",
    "\n",
    "# Select where to train\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on: {device}\\n\")\n",
    "\n",
    "# Load data\n",
    "train_loader,test_loader = load_mnist(batch_size)\n",
    "\n",
    "# Train model\n",
    "start = time()\n",
    "model = train_model(train_loader, test_loader, learning_rate, epochs,device,loss_vec)\n",
    "print(f\"\\nTotal training time: {time()-start} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e428df4",
   "metadata": {},
   "source": [
    "# Print loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data\n",
    "x = range(0, len(loss_vec) * batch_size*100, batch_size*100)\n",
    "y = loss_vec\n",
    "\n",
    "# Define figure\n",
    "fig1 = plt.figure()\n",
    "plt.plot(x,y, label=\"Loss on train data\")\n",
    "plt.xlabel(\"Training examples\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14) \n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Loss while training\", fontsize=16) \n",
    "plt.savefig(\"CNN_SGD_loss.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b025e6",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52caeabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(model,test_loader):\n",
    "    model = model.to(\"cpu\")\n",
    "\n",
    "    # Get model prediction\n",
    "    for data, true_labels in test_loader:        \n",
    "        output = model(data)\n",
    "        _, model_pred = torch.max(output.data, 1)\n",
    "    \n",
    "    # Create confision matrix\n",
    "    labels = [0,1,2,3,4,5,6,7,8,9]\n",
    "    \n",
    "    cm = confusion_matrix(true_labels, model_pred, labels=labels)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.set(font_scale=1.4)\n",
    "    sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, cmap=\"YlGnBu\", fmt='g', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()\n",
    "    plt.savefig(\"confusion_matrix.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc73fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cm(model,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7f233",
   "metadata": {},
   "source": [
    "# Print mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6194ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mistakes(model,test_loader):\n",
    "    model = model.to(\"cpu\")\n",
    "    \n",
    "    # Get model prediction\n",
    "    for data, true_labels in test_loader:        \n",
    "        output = model(data)\n",
    "        _, model_pred = torch.max(output.data, 1)\n",
    "    \n",
    "    # Find mistakes\n",
    "    mistakes = (model_pred != true_labels)  # Logical array where \"True\" represent mistake\n",
    "    mistakes_ids = torch.nonzero(mistakes)   # Extract array of indices of \"True\"\n",
    "    \n",
    "    # Select 5 random indices\n",
    "    random_ids = np.random.choice(len(mistakes_ids), size=5)\n",
    "    \n",
    "    # Store 5 samples in a new array\n",
    "    random_sample = mistakes_ids[random_ids]\n",
    "\n",
    "    # Plot the sample\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for i, idx in enumerate(random_sample):  # i is fig counter, idx is image index\n",
    "        # Data\n",
    "        img = data[idx].reshape(28,28)\n",
    "        true_label = true_labels[idx].item()\n",
    "        predicted_label = model_pred[idx].item()\n",
    "        \n",
    "        # Plot\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f\"True={true_label}  :  Pred={predicted_label}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de391b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mistakes(model,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8215170f",
   "metadata": {},
   "source": [
    "# Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433c8716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is a torch tensor containing all test data: nr_samples, channels, width, height\n",
    "# true_labels is a 1D torch tensor of the correct labels for each sample\n",
    "\n",
    "idx = 261\n",
    "\n",
    "for data, true_labels in test_loader:  \n",
    "    print(f\"data.shape = {data.shape}\")\n",
    "    print(f\"true_labels.shape = {true_labels.shape}\")\n",
    "    print(f\"true_labels[{idx}] = {true_labels[idx]}\")\n",
    "    \n",
    "    output = model(data)\n",
    "    val, model_pred = torch.max(output.data, 1) # return: max.col.val. and index for each row\n",
    "    \n",
    "    print()\n",
    "    print(f\"output.shape = {output.shape}\")\n",
    "    print(f\"output[{idx}] = {output[idx]}\")\n",
    "    print(f\"sum of output[{idx}] = {output[idx].sum()}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"model_pred.shape = {model_pred.shape}\")\n",
    "    print(f\"model_pred[{idx}] = {model_pred[idx]}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"val.shape = {val.shape}\")\n",
    "    print(f\"val[{idx}] = {val[idx]}  (how certain model is for idx {idx})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df9f212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepimg",
   "language": "python",
   "name": "deepimg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
