{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ef31d9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611a7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from time import time\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c864bb",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b836693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(batch_size):\n",
    "\n",
    "    root = \"C:/Users/edvin/GitHub/DeepImg/A3/MNIST/\"\n",
    "    \n",
    "    # Load the dataset\n",
    "    train_data = datasets.MNIST(root+\"Train\", train=True, download=False, transform=ToTensor())\n",
    "    test_data = datasets.MNIST(root+\"Test\", train=False, download=False, transform=ToTensor())\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7576b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: torch.Size([100, 1, 28, 28])\n",
      "\n",
      "  Nr of images: 100\n",
      "  Channels: 1\n",
      "  Width: 28\n",
      "  Height: 28\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_mnist(100)\n",
    "for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "    \n",
    "    print(f\"data.shape: {data.shape}\\n\")\n",
    "    \n",
    "    print(f\"  Nr of images: {data.shape[0]}\")\n",
    "    print(f\"  Channels: {data.shape[1]}\")\n",
    "    print(f\"  Width: {data.shape[2]}\")\n",
    "    print(f\"  Height: {data.shape[3]}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf2b4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: torch.Size([10000, 1, 28, 28])\n",
      "\n",
      "  Nr of images: 10000\n",
      "  Channels: 1\n",
      "  Width: 28\n",
      "  Height: 28\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_mnist(100)\n",
    "for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "    \n",
    "    print(f\"data.shape: {data.shape}\\n\")\n",
    "    \n",
    "    print(f\"  Nr of images: {data.shape[0]}\")\n",
    "    print(f\"  Channels: {data.shape[1]}\")\n",
    "    print(f\"  Width: {data.shape[2]}\")\n",
    "    print(f\"  Height: {data.shape[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd380136",
   "metadata": {},
   "source": [
    "# Define Default Network (for momentum and L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22aaa31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer 1:   8 filters with dim=3x3x1  ->  dim(Z1) = 28x28x8\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1) \n",
    "        \n",
    "        # Layer 2: Max Pooling with dim=2x2    ->  dim(Z2) = 14x14x8\n",
    "        \n",
    "        # Layer 3:   16 filters with dim=3x3x8  ->  dim(Z3) = 14x14x16\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)  # 16 filters: 3x3x8 ->\n",
    "        \n",
    "        # Layer 4: Max Pooling with dim=2x2    ->  dim(Z4) = 7x7x16\n",
    "        \n",
    "        # Layer 5: 32 filters with dim=3x3x16  ->  dim(Z5) =  7x7x32\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) \n",
    "        \n",
    "        # Layer 6: Fully connected with dim=1568x10  ->  dim(Z6) = 1x10\n",
    "        self.W6 = nn.Linear(7 * 7 * 32, 10)   \n",
    "\n",
    "        # Define activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2, stride=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Layer 1\n",
    "        Z1 = self.conv1(X)\n",
    "        A1 = self.relu(Z1)\n",
    "        \n",
    "        # Layer 2\n",
    "        A2 = self.maxpool(A1)\n",
    "        \n",
    "        # Layer 3\n",
    "        Z3 = self.conv2(A2)\n",
    "        A3 = self.relu(Z3)\n",
    "        \n",
    "        # Layer 4\n",
    "        A4 = self.maxpool(A3)\n",
    "        \n",
    "        # Layer 5\n",
    "        Z5 = self.conv3(A4)\n",
    "        A5 = self.relu(Z5)\n",
    "        A5 = A5.reshape(A5.size(0), -1)  # Flatten\n",
    "\n",
    "        # Layer 6\n",
    "        Z6 = self.W6(A5)\n",
    "        Y_hat = self.softmax(Z6)\n",
    "\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d9fd0",
   "metadata": {},
   "source": [
    "# Accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a0525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(output,labels):\n",
    "    # How many data points\n",
    "    n = labels.size(0)\n",
    "    \n",
    "    # Get model predictions\n",
    "    _, predictions = torch.max(output, 1)\n",
    "    \n",
    "    # Vector of \"True\" or \"False\" if prediction matches true labels\n",
    "    correct = (predictions == labels)\n",
    "    \n",
    "    # Count how many \"True\"\n",
    "    total_correct = correct.sum().item()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = total_correct / n\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f537ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(test_loader,model,device):\n",
    "    # Load data and labels\n",
    "    for data, labels in test_loader:  # Test_loader is one batch of all test data\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "    # Feed into model\n",
    "    output = model(data).to(device)\n",
    "    \n",
    "    # Get accuracy by comparing output and labels\n",
    "    acc_test = get_accuracy(output,labels) \n",
    "    \n",
    "    return acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743257d6",
   "metadata": {},
   "source": [
    "# Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ef01976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,test_loader,learning_rate, epochs, device, loss_vec):\n",
    "    \n",
    "    # Define model\n",
    "    model = CNN().to(device)  # Select model and move to \"device\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    \n",
    "    # Performance with random weights\n",
    "    test_acc = test_accuracy(test_loader,model,device)\n",
    "    print(f'Epoch [0/{epochs}],                                  , Test Acc: {test_acc:.5f}')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1,epochs+1):\n",
    "        for idx, (data, labels) in enumerate(train_loader):\n",
    "            # Move to \"device\"\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Model forward and backward\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Save loss every 100 batches\n",
    "            if idx%100==0:\n",
    "                loss_vec.append(loss.item())\n",
    "            \n",
    "        # Accuracy on last mini-batch of train data\n",
    "        acc_train = get_accuracy(output,labels)\n",
    "        \n",
    "        # Accuracy on all test data\n",
    "        acc_test = test_accuracy(test_loader,model,device)  \n",
    "            \n",
    "        # Print epoch results\n",
    "        print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item():.5f}, Train Acc: {acc_train:.5f}, Test Acc: {acc_test:.5f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e82cb7",
   "metadata": {},
   "source": [
    "# Train SGD with momentum=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef1107",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "\n",
      "Epoch [0/15],                                  , Test Acc: 0.06020\n",
      "Epoch [1/15], Loss: 1.49665, Train Acc: 0.97000, Test Acc: 0.95670\n",
      "Epoch [2/15], Loss: 1.49760, Train Acc: 0.97000, Test Acc: 0.96650\n",
      "Epoch [3/15], Loss: 1.51145, Train Acc: 0.95000, Test Acc: 0.97300\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 100\n",
    "learning_rate = 0.05\n",
    "epochs = 15\n",
    "\n",
    "# List for storing loss\n",
    "momentum_loss_vec = []\n",
    "\n",
    "# Select where to train\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on: {device}\\n\")\n",
    "\n",
    "# Load data\n",
    "train_loader,test_loader = load_mnist(batch_size)\n",
    "\n",
    "# Train model\n",
    "start = time()\n",
    "model = train_model(train_loader, test_loader, learning_rate, epochs,device,momentum_loss_vec)\n",
    "print(f\"\\nTotal training time: {time()-start} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b29f7c",
   "metadata": {},
   "source": [
    "# Train with L2 regularizaiton = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1017b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,test_loader,learning_rate, epochs, device, loss_vec):\n",
    "    \n",
    "    # Define model\n",
    "    model = CNN().to(device)  # Select model and move to \"device\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "    \n",
    "    # Performance with random weights\n",
    "    test_acc = test_accuracy(test_loader,model,device)\n",
    "    print(f'Epoch [0/{epochs}],                                  , Test Acc: {test_acc:.5f}')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1,epochs+1):\n",
    "        for idx, (data, labels) in enumerate(train_loader):\n",
    "            # Move to \"device\"\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Model forward and backward\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Save loss every 100 batches\n",
    "            if idx%100==0:\n",
    "                loss_vec.append(loss.item())\n",
    "            \n",
    "        # Accuracy on last mini-batch of train data\n",
    "        acc_train = get_accuracy(output,labels)\n",
    "        \n",
    "        # Accuracy on all test data\n",
    "        acc_test = test_accuracy(test_loader,model,device)  \n",
    "            \n",
    "        # Print epoch results\n",
    "        print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item():.5f}, Train Acc: {acc_train:.5f}, Test Acc: {acc_test:.5f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b69fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 100\n",
    "learning_rate = 0.05\n",
    "epochs = 15\n",
    "\n",
    "# List for storing loss\n",
    "l2_loss_vec = []\n",
    "\n",
    "# Select where to train\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on: {device}\\n\")\n",
    "\n",
    "# Load data\n",
    "train_loader,test_loader = load_mnist(batch_size)\n",
    "\n",
    "# Train model\n",
    "start = time()\n",
    "model = train_model(train_loader, test_loader, learning_rate, epochs,device,l2_loss_vec)\n",
    "print(f\"\\nTotal training time: {time()-start} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18ddf0",
   "metadata": {},
   "source": [
    "# Train with Dropout Layer = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d95874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer 1:   8 filters with dim=3x3x1  ->  dim(Z1) = 28x28x8\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1) \n",
    "        \n",
    "        # Layer 2: Max Pooling with dim=2x2    ->  dim(Z2) = 14x14x8\n",
    "        \n",
    "        # Layer 3:   16 filters with dim=3x3x8  ->  dim(Z3) = 14x14x16\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)  # 16 filters: 3x3x8 ->\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Layer 4: Max Pooling with dim=2x2    ->  dim(Z4) = 7x7x16\n",
    "        \n",
    "        # Layer 5: 32 filters with dim=3x3x16  ->  dim(Z5) =  7x7x32\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) \n",
    "        \n",
    "        # Layer 6: Fully connected with dim=1568x10  ->  dim(Z6) = 1x10\n",
    "        self.W6 = nn.Linear(7 * 7 * 32, 10)   \n",
    "\n",
    "        # Define activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2, stride=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Layer 1\n",
    "        Z1 = self.conv1(X)\n",
    "        A1 = self.relu(Z1)\n",
    "        \n",
    "        # Layer 2\n",
    "        A2 = self.maxpool(A1)\n",
    "        \n",
    "        # Layer 3\n",
    "        Z3 = self.conv2(A2)\n",
    "        A3 = self.relu(Z3)\n",
    "        \n",
    "        # Dropout\n",
    "        A3 = self.dropout(A3)\n",
    "        \n",
    "        # Layer 4\n",
    "        A4 = self.maxpool(A3)\n",
    "        \n",
    "        # Layer 5\n",
    "        Z5 = self.conv3(A4)\n",
    "        A5 = self.relu(Z5)\n",
    "        A5 = A5.reshape(A5.size(0), -1)  # Flatten\n",
    "\n",
    "        # Layer 6\n",
    "        Z6 = self.W6(A5)\n",
    "        Y_hat = self.softmax(Z6)\n",
    "\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b14f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 100\n",
    "learning_rate = 0.05\n",
    "epochs = 15\n",
    "\n",
    "# List for storing loss\n",
    "dropout_loss_vec = []\n",
    "\n",
    "# Select where to train\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on: {device}\\n\")\n",
    "\n",
    "# Load data\n",
    "train_loader,test_loader = load_mnist(batch_size)\n",
    "\n",
    "# Train model\n",
    "start = time()\n",
    "model = train_model(train_loader, test_loader, learning_rate, epochs,device,dropout_loss_vec)\n",
    "print(f\"\\nTotal training time: {time()-start} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e428df4",
   "metadata": {},
   "source": [
    "# Print loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the plot style\n",
    "plt.style.use('default')\n",
    "\n",
    "# Define data\n",
    "x1 = range(0, len(momentum_loss_vec) * batch_size*100, batch_size*100)\n",
    "y1 = momentum_loss_vec\n",
    "\n",
    "x2 = range(0, len(l2_loss_vec) * batch_size*100, batch_size*100)\n",
    "y2 = l2_loss_vec\n",
    "\n",
    "x3 = range(0, len(dropout_loss_vec) * batch_size*100, batch_size*100)\n",
    "y3 = dropout_loss_vec\n",
    "\n",
    "\n",
    "# Clear current plot\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "# Define figure\n",
    "fig1 = plt.figure()\n",
    "\n",
    "plt.plot(x1,y1, label=\"SGD + Momentum=0.9\")\n",
    "plt.plot(x2,y2, label=\"L2 Regularization\")\n",
    "plt.plot(x3,y3, label=\"Dropout layer\")\n",
    "\n",
    "plt.xlabel(\"Training examples\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14) \n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"The loss for three different variations\", fontsize=16) \n",
    "plt.savefig(\"CNN_variations_loss.eps\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepimg",
   "language": "python",
   "name": "deepimg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
