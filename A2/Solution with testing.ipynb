{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24bbca3c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ebc9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import imageio\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ce303",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5498f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    # Loads the MNIST dataset from png images\n",
    " \n",
    "    NUM_LABELS = 10        \n",
    "    # create list of image objects\n",
    "    test_images = []\n",
    "    test_labels = []    \n",
    "    \n",
    "    for label in range(NUM_LABELS):\n",
    "        for image_path in glob.glob(\"MNIST/Test/\" + str(label) + \"/*.png\"):\n",
    "            image = imageio.imread(image_path)\n",
    "            test_images.append(image)\n",
    "            letter = [0 for _ in range(0,NUM_LABELS)]    \n",
    "            letter[label] = 1\n",
    "            test_labels.append(letter)  \n",
    "            \n",
    "    # create list of image objects\n",
    "    train_images = []\n",
    "    train_labels = []    \n",
    "    \n",
    "    for label in range(NUM_LABELS):\n",
    "        for image_path in glob.glob(\"MNIST/Train/\" + str(label) + \"/*.png\"):\n",
    "            image = imageio.imread(image_path)\n",
    "            train_images.append(image)\n",
    "            letter = [0 for _ in range(0,NUM_LABELS)]    \n",
    "            letter[label] = 1\n",
    "            train_labels.append(letter)                  \n",
    "            \n",
    "    X_train= np.array(train_images).reshape(-1,784)/255.0\n",
    "    Y_train= np.array(train_labels)\n",
    "    X_test= np.array(test_images).reshape(-1,784)/255.0\n",
    "    Y_test= np.array(test_labels)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc033a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(X,Y,nr):\n",
    "    fig, axes = plt.subplots(1, nr, figsize=(12,4))\n",
    "    \n",
    "    # Shuffle the data and labels in the same order\n",
    "    shuffle_indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(shuffle_indices)\n",
    "\n",
    "    X = X[shuffle_indices]\n",
    "    Y = Y[shuffle_indices]\n",
    "    \n",
    "    # Plots\n",
    "    for i, ax in enumerate(axes):\n",
    "        img = X[i].reshape(28, 28)\n",
    "        label = np.argmax(Y[i])\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a9fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1094948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_digits(X_train, Y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58af21ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bca5b425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3b264",
   "metadata": {},
   "source": [
    "# Neural Net Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8176ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers, func):\n",
    "    \"\"\"\n",
    "    W and b are standard python lists.\n",
    "    W[i] is the weight matrix for layer i.\n",
    "    b[i is the bias/offset vector for layer i.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_states = []  # List of dictionaries, one dictionary for each layer\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    for i in range(len(layers)-1):\n",
    "        layer_state = {}\n",
    "        \n",
    "        cols = layers[i]\n",
    "        rows = layers[i+1]\n",
    "        \n",
    "        W_i = np.random.normal(loc=0, scale=0.01, size=(rows, cols))\n",
    "        b_i = np.zeros(rows)\n",
    "        \n",
    "        layer_state[\"W\"] = W_i\n",
    "        layer_state[\"b\"] = b_i\n",
    "        \n",
    "        layer_state[\"func\"] = func\n",
    "        \n",
    "        model_states.append(layer_state)\n",
    "        \n",
    "    return model_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0785d",
   "metadata": {},
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5aed281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a31a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d086ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(w_i, x, b_i):\n",
    "    \"\"\"\n",
    "    w_i is a (D1,D2) weight matrix for one layer\n",
    "    x is (D2,1) vector\n",
    "    b_i is a (D1,1) vector\n",
    "    \n",
    "    z is a (D1,1) vector\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.matmul(w_i,x)+b_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "84dd7585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_forward(z,act_func):\n",
    "    \"\"\"\n",
    "    z is a vector, representing a layer\n",
    "    func specifies which activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    if act_func==\"relu\":\n",
    "        return relu(z)\n",
    "    elif act_func == \"sigmoid\":\n",
    "        return sigmoid(z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fa6db828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(model_states, x):\n",
    "    \"\"\"\n",
    "    model_states is list of dictionaries,\n",
    "    where each dictionary containts information about each layer\n",
    "    \n",
    "    x is the input (781,1)\n",
    "    \n",
    "    act_func specifies which activation function to use\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(model_states)):\n",
    "        \n",
    "        # Current layer\n",
    "        layer_state = model_states[i]\n",
    "        \n",
    "        # Get weights and biases\n",
    "        W = layer_state[\"W\"]\n",
    "        b = layer_state[\"b\"]\n",
    "        \n",
    "        # Linear forward\n",
    "        z = linear_forward(W,x,b)\n",
    "        layer_state[\"linear_output\"] = z\n",
    "        \n",
    "        # Activation\n",
    "        z = activation_forward(z,layer_state[\"func\"])\n",
    "        layer_state[\"activation_output\"] = z\n",
    "        \n",
    "        # Step\n",
    "        x=z\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0cfa71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    z is the models output represented by a (10,1) vector\n",
    "    \"\"\"\n",
    "    z_exp = np.exp(z)\n",
    "    return z_exp/np.sum(z_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158eef0",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "98663bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(z,y):\n",
    "    \"\"\"\n",
    "    Loss according to cross entropy, eq.2\n",
    "    \n",
    "    z is the model output as vector with size (10,1)\n",
    "    y is the one hot encoding true label with size (10,1)\n",
    "    \n",
    "    Subtract maximum to increase stability. Change in loss (+-4.440892098500626e-16)\n",
    "    \"\"\"\n",
    "\n",
    "    z = z - max(z)\n",
    "    return np.log(np.sum(np.exp(z))) - np.dot(z,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "105c85df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z,Y):\n",
    "    # Nr of datapoints\n",
    "    n = len(Y) \n",
    "    \n",
    "    # Total cost\n",
    "    cost = 0\n",
    "    for i in range(n):\n",
    "        cost += compute_loss(Z[i],Y[i])\n",
    "        \n",
    "    # Average cost\n",
    "    avg_cost = cost/n\n",
    "        \n",
    "    return avg_cost\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b565ff",
   "metadata": {},
   "source": [
    "### Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d156cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(layer_state, X, Y):\n",
    "    \"\"\"\n",
    "    layer_state is a dictionary containing the layer state information\n",
    "    X is the input to the layer\n",
    "    Y is the true output of the layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read layer_state\n",
    "    z = layer_state[\"linear_output\"]\n",
    "    z_act = layer_state[\"activation_output\"]\n",
    "    w = layer_state[\"W\"]\n",
    "    b = layer_state[\"b\"]\n",
    "    \n",
    "    # Backprop\n",
    "    db = np.mean(softmax(z)-Y)\n",
    "    dW = np.mean(np.dot(softmax(z)-Y)*X)\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5e753dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(X):\n",
    "    \"\"\"\n",
    "    x is a number that is the input to the sigmoid\n",
    "    \"\"\"\n",
    "    return sigmoid(X) * (1-sigmoid(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "682b1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(X):\n",
    "    \"\"\"\n",
    "    x is a number that is the input to the relu\n",
    "    \"\"\"\n",
    "    if X>0:\n",
    "        derivative = 1\n",
    "    else:\n",
    "        derivative = 0\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "27dd06c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_backward(layer_state, X, Y):\n",
    "    func = layer_state[\"func\"]\n",
    "    \n",
    "    if func == \"sigmoid\":\n",
    "        return sigmoid_backward(X)*linear_backward(layer_state, X, Y)\n",
    "    elif func == \"relu\":\n",
    "        return relu_backward(X)*linear_backward(layer_state, X, Y)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "03a3d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_backward(model_states, x, y):\n",
    "    nr_layers = len(model_states)\n",
    "    \n",
    "    for i in range(nr_layers):\n",
    "        layer_state = model_states[nr_layers-1]\n",
    "        dW,db = activation_backward(layer_state,x,y)\n",
    "        \n",
    "        layer_state[\"dW\"] = dW\n",
    "        layer_state[\"db\"] = db\n",
    "        \n",
    "    return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "06c9f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(model_states, learning_rate):\n",
    "    \n",
    "    for i in range(len(model_states)):\n",
    "        layer_state = model_states[i]\n",
    "        layer_state[\"W\"] = layer_state[\"W\"] - learning_rate*layer_state[\"dW\"]\n",
    "        layer_state[\"b\"] = layer_state[\"b\"] - learning_rate*layer_state[\"db\"]\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f03488",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a1a2bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_states, X, Y):\n",
    "    model_output = np.zeros(len(Y))\n",
    "    \n",
    "    correct = 0\n",
    "    for i in range(len(Y)):\n",
    "        model_output[i] = np.argmax(model_forward(model_states, X[i]))\n",
    "        true_output = np.argmax(Y[i])\n",
    "        \n",
    "        if model_output[i] == true_outout:\n",
    "            correct += 1\n",
    "    \n",
    "    acc = correct/len(Y)\n",
    "        \n",
    "    return model_output, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5353a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X,Y,batch_size):\n",
    "    mini_batches = []\n",
    "    nr_datapoints = len(X)\n",
    "    nr_batches = nr_datapoints//batch_size\n",
    "    \n",
    "    # Shuffle X and Y\n",
    "    shuffle_indices = np.random.permutation(nr_datapoints)\n",
    "    X = X[shuffle_indices]\n",
    "    Y = Y[shuffle_indices]\n",
    "    \n",
    "    # Organize into mini-batches\n",
    "    for i in range(nr_batches):\n",
    "        start = i*batch_size\n",
    "        end = (i+1)*batch_size\n",
    "        \n",
    "        x_mini = X[start:end]\n",
    "        y_mini = Y[start:end]\n",
    "        mini_batches.append((x_mini, y_mini))\n",
    "    \n",
    "    # Remainder\n",
    "    if nr_datapoints % batch_size != 0:\n",
    "        start = nr_batches*batch_size\n",
    "        \n",
    "        x_mini = X[start:]\n",
    "        y_mini = Y[start:]\n",
    "        mini_batches.append((x_mini, y_mini))\n",
    "    \n",
    "    \n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468af02",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ffc505",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [784,128,64,32,10]\n",
    "trained_model = train_model(X_train, Y_train, layers, \"sigmoid\", 500, 0.01, 100, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f1573e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, layers, act_func, iterations, learning_rate, batch_size, X_test, Y_test):\n",
    "    \n",
    "    # Initialize\n",
    "    model_states = initialize_parameters(layers, act_func):\n",
    "        \n",
    "    # Train\n",
    "    for i in range(iterations):\n",
    "        mini_batches = random_mini_batches(X_train, Y_train)\n",
    "        \n",
    "        for x,y in mini_batches:  # Each x,y pair is a datapoint (image with label)\n",
    "            # Forward pass in network\n",
    "            z = model_forward(model_states,x)\n",
    "            \n",
    "            # Backpropogation\n",
    "            model_backward()\n",
    "            \n",
    "            # Update parameters\n",
    "            update_parameters(model_states, learning_rate):\n",
    "            \n",
    "\n",
    "        if i%10==0:\n",
    "            Z, acc = predict(model_state,X_test,Y_test)\n",
    "            cost = compute_cost(Z,Y_test)\n",
    "            print(\"Cost = \", cost)\n",
    "            print(\"Accuracy = \", acc)\n",
    "        \n",
    "    \n",
    "    return model_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66822415",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e93adb",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "70a1ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [784,128,64,32,10]\n",
    "model_states = initialize_parameters(layers, \"sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82797402",
   "metadata": {},
   "source": [
    "### Linear Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "48d9bd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer\n",
      " x =  (784,)\n",
      " w =  (128, 784)\n",
      " b =  (128,)\n",
      "\n",
      " z =  (128,)\n",
      "\n",
      "\n",
      "Second layer\n",
      " z =  (128,)\n",
      " w =  (64, 128)\n",
      " b =  (64,)\n",
      "\n",
      " z =  (64,)\n"
     ]
    }
   ],
   "source": [
    "print(\"First layer\")\n",
    "layer_state = model_states[0]\n",
    "x = X_train[0]\n",
    "\n",
    "print(\" x = \", x.shape)\n",
    "print(\" w = \", layer_state[\"W\"].shape)\n",
    "print(\" b = \", layer_state[\"b\"].shape)\n",
    "z = linear_forward(layer_state[\"W\"], x, layer_state[\"b\"])\n",
    "print(\"\\n z = \", z.shape)\n",
    "\n",
    "\n",
    "print(\"\\n\\nSecond layer\")\n",
    "layer_state = model_states[1]\n",
    "\n",
    "print(\" z = \", z.shape)\n",
    "print(\" w = \", layer_state[\"W\"].shape)\n",
    "print(\" b = \", layer_state[\"b\"].shape)\n",
    "z = linear_forward(layer_state[\"W\"], z, layer_state[\"b\"])\n",
    "print(\"\\n z = \", z.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a290b12",
   "metadata": {},
   "source": [
    "### Activation forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "31c575df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid =  [0.52497919 0.42555748 0.66818777 0.71300016]\n",
      "relu =  [0.1  0.   0.7  0.91]\n"
     ]
    }
   ],
   "source": [
    "# Correctness\n",
    "# Z = [0.1, -0.3, 0.7, 0.91]   --sigmoid--> [0.524979, 0.425557, 0.668188, 0.713]\n",
    "# Z = [0.1, -0.3, 0.7, 0.91]   ----relu---> [0.1, 0, 0.7, 0.91]\n",
    "\n",
    "z = np.array([0.1, -0.3, 0.7, 0.91])\n",
    "print(\"sigmoid = \", activation_forward(z,\"sigmoid\"))\n",
    "print(\"relu = \", activation_forward(z,\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d327b",
   "metadata": {},
   "source": [
    "### Model forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0a00c17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer\n",
      " x =  (784,)\n",
      "Output layer\n",
      " z =  (10,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input layer\")\n",
    "x = X_train[250]\n",
    "print(\" x = \", x.shape)\n",
    "\n",
    "print(\"Output layer\")\n",
    "z = model_forward(model_states,x,'sigmoid')\n",
    "print(\" z = \", z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b4c6d957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_states[3][\"W\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a0c56",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dff96c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25949646 0.70538451 0.03511903]\n",
      "sum =  1.0\n"
     ]
    }
   ],
   "source": [
    "# Correctness\n",
    "# z = [3,4,1]    --softmax-->   [0.2595, 0.7054, 0.0351]\n",
    "\n",
    "z = [3,4,1]\n",
    "out = softmax(z)\n",
    "print(out)\n",
    "print(\"sum = \", sum(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f12c5",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d86dadcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model pred =  [0.50069566 0.50015321 0.50350779 0.49696496 0.4944279  0.51236655\n",
      " 0.49870593 0.49944178 0.49107393 0.50704301]\n",
      "true label =  [1 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Cross entropy loss =  2.3023442901184428\n",
      "Softmax+NLL loss =  2.3023442901184423\n"
     ]
    }
   ],
   "source": [
    "# Cross entropy loss (the loss function implemented) is the same as NLL on softmax\n",
    "\n",
    "x0 = X_train[0]\n",
    "y_true = Y_train[0]\n",
    "\n",
    "z = model_forward(model_states,x0)\n",
    "loss = compute_loss(z,y_true)  # loss =  2.3014864872668612\n",
    "\n",
    "out = softmax(z)\n",
    "loss2 = -np.log(out)\n",
    "\n",
    "print(\"model pred = \", z)\n",
    "print(\"true label = \", y_true)\n",
    "print()\n",
    "print(\"Cross entropy loss = \", loss)\n",
    "print(\"Softmax+NLL loss = \", loss2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aada533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
